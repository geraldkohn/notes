# 场景题

## Top K 问题

最经典的解法是利用堆，如果要求前 K 个最大的数，那么维护一个大小为 K 的大顶堆即可，如果要求前 K 个最小的数，那么维护一个大小为 K 的小顶堆即可。

在整个操作中，遍历数组需要 O(n) 的时间复杂度，一次堆化需要 O(logK)，加起来就是 O(nlogK) 的复杂度。

## 海量数据处理

### 方法一：hash分治+HashMap+堆排

   所谓海量数据处理，无非就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。

    那解决办法呢?针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如Bloom filter/Hash/bit-map/堆/数据库或倒排索引/trie树，针对空间，无非就一个办法：大而化小，分而治之（hash映射），你不是说规模太大嘛，那简单啊，就把规模大化为规模小的，各个击破不就完了嘛。

    至于所谓的单机及集群问题，通俗点来讲，单机就是处理装载数据的机器有限(只要考虑cpu，内存，硬盘的数据交互)，而集群，机器有多辆，适合分布式处理，并行计算(更多考虑节点和节点间的数据交互)。

    再者，通过本blog内的有关海量数据处理的文章：[Big Data Processing](http://blog.csdn.net/v_july_v/article/category/1106578)，我们已经大致知道，处理海量数据问题，无非就是：

1. 分而治之/hash映射 + hash统计 + 堆/快速/归并排序；
2. 双层桶划分
3. Bloom filter/Bitmap；
4. Trie树/数据库/倒排索引；
5. 外排序；
6. 分布式处理之Hadoop/Mapreduce。

#### 题目一：

> **有10个海量日志数据文件，提取出某日访问百度次数最多的Top K IP**

1. 将10个大文件分成n个小文件。先建立n个小文件，使用Hash函数，让key相同的落在同一个文件中。（将 hash(key) % n 相同的写入同一个文件中）

2. 使用HashMap统计小文件中的 (key,value) 

3. 使用堆排序来得到小文件的 Top K

4. 然后将使用堆排序来聚合小文件中的 Top K

找到小文件中 Top K （单机 Top K）时间复杂度：O(M) + O(N * logk) 

> M：总共记录的条数
> 
> N：总共记录下来的 IP 个数
> 
> O(M)：使用 hashmap 来聚合记录，得出每一个 IP 都出现了几次。
> 
> O(logk)：一个元素插入到堆中的时间复杂度，N 就是 N 个 IP 都需要插入一遍。

#### 题目二：

> **寻找热门查询，300万个查询字符串中统计最热门的10个查询**

如题目一

#### 题目三：

> **有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。**

如题目一

#### 题目四：

> **海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。**

1. 堆排序：求每一台电脑的 Top K
2. 将电脑中数据的 Top K 使用 HashMap 组合起来，再使用堆排序。

#### 题目五：

> **有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。**

如题目一。

### 参考：

 [十道海量数据处理面试题与十个方法大总结](http://blog.csdn.net/v_JULY_v/archive/2011/03/26/6279498.aspx)

[教你如何迅速秒杀掉：99%的海量数据处理面试题_牛客博客](https://blog.nowcoder.net/n/eb5599d1c1f147a3946df56302b25016)
